
$$
\DeclareMathOperator{\Img}{im}
$$

# 函数
# Functions

函数是不同集合间值的映射关系:

Fundamentally, a function is a relationship (mapping) between the values of some set $X$ and some set $Y$:

$$ f:X \to Y $$

![A function is a mapping between domains.](assets/function.svg)

函数能对同一个集合内的元素自身映射.例如, $f(x) = x^2$,也可记为$f:x \mapsto x^2$, 是所有实数映射到所有实数, $f:\mathbb R \to \mathbb R$.

A function can map a set to itself. For example, $f(x) = x^2$, also notated $f:x \mapsto x^2$, is the mapping of all real numbers to all real numbers, or $f:\mathbb R \to \mathbb R$.

映射的源集合叫 **定义域**.

The set you are mapping _from_ is called the __domain__.

映射的目标集合叫**到达域**.

The set that is being mapped _to_ is called the __codomain__.

**值域**是到达域的子集, 是函数真正映射到的范围.(一个函数不一定会映射到到达域中的每一个值, 但当这发生时, 值域与到达域相等)

The __range__ is the subset of the codomain which the function actually maps to (a function doesn't necessarily map to _every_ value in the codomain. But where it does, the range equals the codomain).

映射到$\mathbb R$的函数称为**标量函数**或**实数函数**.

Functions which map to $\mathbb R$ are known as __scalar-valued__ or __real-valued__ functions.

映射到$\mathbb R^n$的函数,当$n > 1$时,是**向量函数**.

Functions which map to $\mathbb R^n$ where $n > 1$ are known as __vector-valued__ functions.

### 恒等函数
### Identity functions

恒等函数是把值映射到自身的函数:

An identity function maps something to itself:

$$
I_X : X \to X
$$

也就是说, 对$X$中的每个$a$,$I_X(a) = a$:

That is, for every $a$ in $X$, $I_X(a) = a$:

$$
I_X(a) = a, \forall \, a \in X
$$

### 反函数
### The inverse of a function

例如说有一个函数$f: X \to Y$, 对任意$a \in X$,有$f(a) = b$.
Say we have a function $f: X \to Y$, where $f(a) = b$ for any $a \in X$.

我们说$f$是**可逆的**, 如果存在一个函数$f^{-1}: Y \to X$,使 $f^{-1} \circ f = I_X$ and $f \circ f^{-1} = I_Y$,

We say $f$ is __invertible__ if and only if there exists a function $f^{-1}: Y \to X$ such that $f^{-1} \circ f = I_X$ and $f \circ f^{-1} = I_Y$. Note that $\circ$ denotes __function composition__, i.e. $f \circ g = f(g)$, which is the same as $f(g(x))$.

函数的逆是具有**唯一性**,它是满射且是一一映射,每一个$y$都有唯一的$x$与之对应.

The inverse of a function is _unique_, that is, it is _surjective_ and _injective_ (described below), that is, there is a unique $x$ for each $y$.

### 满射
### Surjective functions

**满射**函数，是一个函数$f: X \to Y$ ，对每个$y \in Y$存在_至少_一个$x \in X$,使$f(x) = y$.

A __surjective__ function, also called "onto", is a function $f: X \to Y$ where, for every $y \in Y$ there exists _at least_ one $x \in X$ such that $f(x) = y$. That is, every $y$ has at least one corresponding $x$ value.

等价于:

This is equivalent to:

$$ \text{range}(f) = Y $$

### 单射
### Injective functions

**单射**函数，是一个函数$f: X \to Y$, 对每个$y \in Y$存在_至多_一个$x \in X$,使$f(x) = y$.

An __injective__ function, also called "one-to-one", is a function $f: X \to Y$ where, for every $y \in Y$, there exists _at most_ one $x \in X$ such that $f(x) = y$.

也就是说，不是全部 $y$ 都必然具有对应的$X$, 但有对应的都只有一个$x$.

That is, not all $y$ necessarily has a corresponding $x$, but those that do only have _one_ corresponding $x$.

### 满射与单射
### Surjective & injective functions

一个函数可以同时是满射和单射，这只是意味着对每个$y \in Y$存在都有且只有一个$x \in X$,使$f(x) = y$.

A function can be both surjective and injective, which just means that for every $y \in Y$ there exists exactly one $x \in X$ such that $f(x) = y$, that is, every $y$ has exactly one corresponding $x$.

如前所述，函数的逆函数就是满射和单射的！

As mentioned before, the inverse of a function is both surjective and injective!

### 凸函数与非凸函数
### Convex and non-convex functions

> 凸函数是一个连续函数，其域中每个间隔中点的值不超过这间隔两端的算术平均值。([Convex Function](http://mathworld.wolfram.com/ConvexFunction.html).Weisstein, Eric W. Wolfram MathWorld

> A convex function is a continuous function whose value at the midpoint of every interval in its domain does not exceed the arithmetic mean of its values at the ends of the interval. ([Convex Function](http://mathworld.wolfram.com/ConvexFunction.html). Weisstein, Eric W. Wolfram MathWorld)

一个**凸**域是在该区域中任何两点都可以在不离开该区域情况下用直线连接。

A __convex__ region is one in which any two points in the region can be joined by a straight line that does not leave the region.

这就是说，凸函数具有且只有一个最小值（这也是导数为0的唯一位置）。

Which is to say that a convex function has a minimum, and only one (and this is also the only position where the derivative is 0).

更正式地说，如果一个函数的二阶导数总是正值，那这个函数就是凸函数。一个函数也可以仅在范围$[a,b]$上为凸的,只要它的二阶导数在该范围内的任何地方都是正的。

More formally, a function is convex if the second derivative is positive everywhere. A function can be convex on a range $[a,b]$ if its second derivative is positive everywhere in that range.

在更高的维度，这些导数不是标量，所以我们反而定义凸这个性质为如果_Hessian_ $H$（二阶导数的矩阵）是半正定（记为$H \succeq 0$）,这是严格的，如果凸HH是正定（记为$H \succ 0$）。更多细节请参考微积分部分。

In higher dimensions, these derivatives aren't scalar values, so we instead define convexity if the _Hessian_ $H$ (the matrix of second derivatives) is _positive semidefinite_ (notated $H \succeq 0$). It is _strictly_ convex if $H$ is _positive definite_ (notated $H \succ 0$). Refer to the Calculus section for more details on this.

![Convex and non-convex functions](assets/convex_nonconvex.svg)

### 超越函数
### Transcendental functions

超越函数是那些非多项式函数,如:$\sin, \exp, \log, \text{etc}$.


__Transcendental__ functions are those that are not polynomial, e.g. $\sin, \exp, \log, \text{etc}$.

### 对数函数
### Logarithms

对数函数会经常遇到, 它们有很多有用的特性, 例如可以把乘法转换为加法.

Logarithms are frequently encountered. They have many useful properties, such as turning multiplication into addition:

$$
\log(xy) = \log(x) + \log(y)
$$

在计算机运算中, 乘以非常小的数是个问题，因为会导致下溢错误。对数函数通常用于将这种乘法转换为加法，以避免下溢错误。

Multiplying many small numbers is problematic with computers, leading to underflow errors. Logarithms are commonly used to turn this kind of multiplication into addition and avoid underflow errors.

注意 $\log(x)$，在没有指定基底时，通常指自然对数，即 $\log_e(x)$，有时记为 $\ln(x)$），它的逆函数是$\exp(x)$,更多记为$e^x$。

Note that $\log(x)$, without any base, typically implies the natural log, i.e. $\log_e(x)$, sometimes notated $\ln(x)$, which has the inverse $\exp(x)$, more commonly seen as $e^x$.

# 其他概念
# Other useful concepts

## 解析法与数值法
## Solving analytically vs numerically

一般你会看到以**解析方法**(有时称为**代数方法**)与**数值方法**去解决问题有很大区别.

Often you may see a distinction made between solving a problem __analytically__ (sometimes __algebraeically__ is used) and solving a problem __numerically__.

以分析法解决问题意味着您可以利用对象和等式的属性，例如通过微积分中的方法，避免用数值替换您正在操作的变量（即，你只需要操作符号）。如果一个问题可以用解析法求解，那这个解被称为**解析解**（或**分析*解）,它一个精确的解。

Solving a problem analytically means you can exploit properties of the objects and equations, e.g. through methods from calculus, avoiding substituting numerical values for the variables you are manipulating (that is, you only need to manipulate symbols). If a problem may be solved analytically, the resulting solution is called a __closed form__ solution (or the __analytic__ solution) and is an exact solution.


不是所有的问题都可以用解析法; 通常更复杂的数学模型没有闭型解。这些问题也常常是最有趣的。这样的问题需要近似的数值法，它通过对变量代入不同的数值许多次,来算出近似值。这个解称为**近似**（**数值**）解。

Not all problems can be solved analytically; generally more complex mathematical models have no closed form solution. These problems are also often the ones of most interest. Such problems need to be _approximated_ numerically, which involves evaluating the equations many times by substituting different numerical values for variables. The result is an approximate (__numerical__) solution.

## 线性模型与非线性模型
## Linear vs nonlinear models

你经常会看到类似的警告说，某算法只适用于线性模型。另一方面，一些模型也因为适用于非线性模型而受欢迎。

You'll often see a caveat with algorithms that they only work for linear models. On the other hand, some models are touted for their capacity for nonlinear models.

线性模型的一般形式为：

A __linear model__ is a model which takes the general form:

$$
y = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n
$$

请注意，此函数不需要产生字面上的直线。“线性”约束不适用于预测变量$x_1, \dots, x_n$。例如，函数$y = x^2$是线性的。

Note that this function does not need to produce a literal line. The "linear" constraint does not apply to the predictor variables $x_1, \dots, x_n$. For instance, the function $y = x^2$ is linear.

“线性”是指参数; 即函数必须是“在参数中线性”，意味着参数$\beta_0, \dots, \beta_n$ 它们自己必须形成一条线（或者在任何维度空间中的等价物）。

"Linear" refers to the parameters; i.e. the function must be "linear in the parameters", meaning that the parameters $\beta_0, \dots, \beta_n$ themselves must form a line (or its equivalent in whatever dimensional space you're working in).

一个**非线性模型**包括，带有如$\beta^2$ 或$\beta_0 \beta_1$这样的参数（即，同时有多个参数,就是非线性的）或超越函数。

A __nonlinear model__ includes parameters such as $\beta^2$ or $\beta_0 \beta_1$ (that is, multiple parameters in the same term, which is _not_ linear) or transcendental functions.

## 度量
## Metrics

许多人工智能和机器学习算法是基于或受益于某种度量。在本中，该术语有具体的定义。

Many artificial intelligence and machine learning algorithms are based on or benefit from some kind of _metric_. In this context the term has a concrete definition.

度量的典型例子是相似性。假设你有一堆随机变量$X_i$ ,它接受标签空间$V$中的值 。如果$X_i$ 和 $X_j$ 通过边连接，我们要求它们的“相似”度。

The typical case for metrics is around similarity. Say you have a bunch of random variables $X_i$ which take on values in a label space $V$. If $X_i$ and $X_j$ are connected by an edge, we want them to take on "similar" values.

我们如何定义“相似”？

How do we define "similar"?

我们将使用距离函数 $\mu: V \times V \to R^+$，满足：

- **自反性**: 对所有$v$有$\mu(v,v)=0$ 
- **对称性**: 对所有$v_1, v_2$ 有 $\mu(v_1,v_2)=\mu(v_2, v_1)$
- **三角不等性**: 对所有$v_1, v_2, v_3$, 有 $\mu(v_1, v_2) \leq \mu(v_1, v_3) + \mu(v_3, v_2)$  


We'll use a distance function $\mu: V \times V \to R^+$, which needs to satisfy:
- _reflexivity_: $\mu(v,v)=0$ for all $v$
- _symmetry_: $\mu(v_1,v_2)=\mu(v_2, v_1)$ for all $v_1, v_2$
- _triangle inequality_: $\mu(v_1, v_2) \leq \mu(v_1, v_3) + \mu(v_3, v_2)$ for all $v_1, v_2, v_3$

如果所有这些都得到满足，我们说 $\mu$ 是一个度量。

If all these are satisfied, we say that $\mu$ is a __metric__.

如果只自反性和对称性满足，我们用半度量代替。

If only reflexivity and symmetry are satisfied, we have a __semi-metric__ instead.

因此，我们可以创建一个特性 $f_{ij}(X_i, X_j) = \mu(X_i, X_j)$, 然后使：

So we can create a _feature_ $f_{ij}(X_i, X_j) = \mu(X_i, X_j)$ and then this works out such that:

$$
\exp(- w_{ij} f_{ij} (X_i, X_j)), w_{ij} > 0
$$

距离（度量）越低，概率越高。

that the lower the distance (metric), the higher the probability.

## 引用
## References

- [Convex Function](http://mathworld.wolfram.com/ConvexFunction.html). Weisstein, Eric W. Wolfram MathWorld.
